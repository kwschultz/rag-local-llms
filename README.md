## Retrieval Augmented Generation with local LLMs

### Advanced RAG, 'locally' on Google Colab via HuggingFace 🤗 
`Goal`: Complex PDF question answering, many pages including figures. 
- LlamaIndex pipelines, semantic partitioning, re-ranking, response synthesis
- Efficient LLM : [HuggingFace/TinyLlama](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)
- End to end Evaluation: [DeepEval](https://huggingface.co/Efficient-Large-Model/VILA-7b)
- Next experiment: Efficient Multi-modal RAG for more complex visual QA [HuggingFace/VILA](https://huggingface.co/Efficient-Large-Model/VILA-7b)

### Vanilla RAG, locally on M1 macbook.
`Goal`: Privacy-preserving 🤫 sensistive document question answering 📄.
- Ollama + Mistral LLM, Chroma DB 🍭, Nomic Embeddings 🍪
- Langchain 🦜🔗 pipeline

---
"People who buy things are suckers." - Ron Swanson 

