## Retrieval Augmented Generation with local LLMs

### Advanced RAG, 'locally' on Google Colab via HuggingFace ğŸ¤— 
`Goal`: Complex PDF question answering, many pages including figures. 
- LlamaIndex pipelines, semantic partitioning, re-ranking, response synthesis
- Efficient LLM : [HuggingFace/TinyLlama](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)
- End to end Evaluation: [DeepEval](https://huggingface.co/Efficient-Large-Model/VILA-7b)
- Next experiment: Efficient Multi-modal RAG for more complex visual QA [HuggingFace/VILA](https://huggingface.co/Efficient-Large-Model/VILA-7b)

### Vanilla RAG, locally on M1 macbook.
`Goal`: Privacy-preserving ğŸ¤« sensistive document question answering ğŸ“„.
- Ollama + Mistral LLM, Chroma DB ğŸ­, Nomic Embeddings ğŸª
- Langchain ğŸ¦œğŸ”— pipeline

---
"People who buy things are suckers." - Ron Swanson 

